{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# SKlearn imports:\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, f1_score, balanced_accuracy_score, RocCurveDisplay, recall_score, precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay, roc_auc_score, precision_score, confusion_matrix\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df = pd.read_csv('./data/titles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression from the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get title as feature and subreddit as target (Chess or AnarchyChess)\n",
    "X = df['title']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 CountVectorizer(max_features=1000, stop_words='english')),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(max_iter=1000, penalty='none'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using stop words\n",
    "cvect_pipe = make_pipeline(CountVectorizer(stop_words = 'english', max_features = 1000),\n",
    "                           LogisticRegression(max_iter=1000, penalty=\"none\"))\n",
    "\n",
    "cvect_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.644"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>question</td>\n",
       "      <td>-549.780017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>-480.358397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>missed</td>\n",
       "      <td>-474.938330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>puzzles</td>\n",
       "      <td>-447.926010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>books</td>\n",
       "      <td>-441.656092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>titled</td>\n",
       "      <td>-429.512434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>questions</td>\n",
       "      <td>-425.247453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>learning</td>\n",
       "      <td>-413.705266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>equal</td>\n",
       "      <td>-412.919584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>ratings</td>\n",
       "      <td>-410.774303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>royale</td>\n",
       "      <td>-394.892891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>early</td>\n",
       "      <td>-389.741417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>-388.672446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>rating</td>\n",
       "      <td>-383.714891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>advantage</td>\n",
       "      <td>-373.385513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words       coefs\n",
       "513   question -549.780017\n",
       "49   beautiful -480.358397\n",
       "316     missed -474.938330\n",
       "507    puzzles -447.926010\n",
       "69       books -441.656092\n",
       "838     titled -429.512434\n",
       "514  questions -425.247453\n",
       "271   learning -413.705266\n",
       "158      equal -412.919584\n",
       "537    ratings -410.774303\n",
       "613     royale -394.892891\n",
       "145      early -389.741417\n",
       "10        2017 -388.672446\n",
       "536     rating -383.714891\n",
       "26   advantage -373.385513"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling coefs out of pipe (these words help idenify which subreddit a title belongs to)\n",
    "coefs = cvect_pipe['logisticregression'].coef_\n",
    "\n",
    "word_coefs = pd.DataFrame({'words': cvect_pipe['countvectorizer'].get_feature_names_out(), \n",
    "              'coefs': coefs[0]})\n",
    "\n",
    "word_coefs.sort_values(by='coefs', ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>tinder</td>\n",
       "      <td>555.196679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>according</td>\n",
       "      <td>552.160220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>simulated</td>\n",
       "      <td>530.853516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>going</td>\n",
       "      <td>463.075240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>incredible</td>\n",
       "      <td>456.593394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>mates</td>\n",
       "      <td>450.112569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>talks</td>\n",
       "      <td>430.452816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>run</td>\n",
       "      <td>428.422373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>finally</td>\n",
       "      <td>421.143744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50</td>\n",
       "      <td>416.710574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>meme</td>\n",
       "      <td>411.859150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>loses</td>\n",
       "      <td>409.031519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>chat</td>\n",
       "      <td>389.899857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>org</td>\n",
       "      <td>389.442553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>fuck</td>\n",
       "      <td>380.123275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words       coefs\n",
       "834      tinder  555.196679\n",
       "20    according  552.160220\n",
       "684   simulated  530.853516\n",
       "203       going  463.075240\n",
       "239  incredible  456.593394\n",
       "305       mates  450.112569\n",
       "798       talks  430.452816\n",
       "615         run  428.422373\n",
       "173     finally  421.143744\n",
       "19           50  416.710574\n",
       "308        meme  411.859150\n",
       "290       loses  409.031519\n",
       "89         chat  389.899857\n",
       "370         org  389.442553\n",
       "182        fuck  380.123275"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefs.sort_values(by='coefs', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 CountVectorizer(max_features=1000, ngram_range=(1, 2),\n",
       "                                 stop_words='english')),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(max_iter=1000, penalty='none'))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying N-gram\n",
    "cvect_pipe_ngram = make_pipeline(CountVectorizer(stop_words = 'english', max_features = 1000, ngram_range=(1,2)),\n",
    "                           LogisticRegression(max_iter=1000, penalty=\"none\"))\n",
    "\n",
    "cvect_pipe_ngram.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect_pipe_ngram.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.636"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect_pipe_ngram.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 CountVectorizer(max_features=1000, stop_words='english')),\n",
       "                ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect_pipe2 = make_pipeline(CountVectorizer(stop_words = 'english', max_features = 1000),\n",
    "                           MultinomialNB())\n",
    "\n",
    "cvect_pipe2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect_pipe2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.632"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect_pipe2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 CountVectorizer(max_features=5000, ngram_range=(1, 3),\n",
       "                                 stop_words='english')),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(n_estimators=1000, random_state=123))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_pipe = make_pipeline(CountVectorizer(stop_words = 'english', max_features = 5000, ngram_range = (1,3)),\n",
    "                           RandomForestClassifier(n_estimators = 1000, random_state = 123))\n",
    "\n",
    "forest_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933333333333333"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.664"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_pipe.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
